# ğŸš€ Processor Trends and Parallel Computing ğŸš€
ğŸš€ Kicking Off the GPU Journey

> ğŸ‰ **Welcome to the first step of GPU mastery challenge!**  
> We're diving deep into the world of parallel computing, starting from the very basics â€” buckle up and let's make some cores roar! âš¡

## ğŸ“ˆ Processor Evolution: From Moore's Law to Multi-core Era

### Moore's Law and Its Implications ğŸ§©

![Moore's Law](https://upload.wikimedia.org/wikipedia/commons/0/00/Moore%27s_Law_Transistor_Count_1970-2020.png)

**Moore's "Law"** ğŸ“Š predicted that the number of **transistors** per unit area would double every 18-24 months. This observation, made by Intel co-founder Gordon Moore in 1965, has shaped the semiconductor industry for decades.

#### Historical Progression â³

1. **Transistor Density** ğŸ”: Following Moore's prediction, chip manufacturers consistently increased the number of transistors packed onto silicon chips.

2. **Frequency Scaling** âš¡: Until around 2005, processor **frequency** (clock rate) increased proportionally because smaller transistors could be switched faster.

3. **The Power Wall** ğŸ”¥: Around 2005, frequency scaling hit the **power wall** - increasing frequency further would generate too much heat to cool effectively.

4. **Single-Thread Performance Plateau** ğŸ“‰: With frequency stagnation came a flattening of **single-thread performance** gains, with modest improvements only coming from architectural optimizations and compiler advancements.

![Processor_Trends](../materials/images/Chapter_01/Processor_Trends.png)

### The Multi-core Solution ğŸ§ 

As single-core performance improvements slowed, processor manufacturers shifted their focus to multi-core designs, placing multiple processing cores on a single chip. This transition marked the beginning of the parallel computing era for mainstream computing.

## ğŸ—ï¸ Design Philosophies: Latency vs. Throughput

### Latency-Oriented Design ğŸï¸

**Focus: Minimize the time it takes to perform a single task**

![Latency-Oriented Design](../materials/images/Chapter_01/Latency_Oriented_Design.png)


### Throughput-Oriented Design ğŸš‚

**Focus: Maximize the number of tasks that can be performed in a given time frame**

![Throughput-Oriented Design](../materials/images/Chapter_01/Throughput_Oriented_Design.png)

## ğŸ§© CPU vs. GPU: Architectural Comparison

### CPU: Latency-Oriented Design ğŸ’¨

- âœ… A few powerful **ALUs** (Arithmetic Logic Units)
- âœ… Reduced operation latency
- âœ… Large **caches** to convert long latency memory accesses to short latency cache accesses
- âœ… Sophisticated **control** systems:
  - Branch prediction to reduce control hazards
  - Data forwarding to reduce data hazards
- âœ… Modest multithreading to hide short latency
- âœ… High clock frequency

![CPU Design](../materials/images/Chapter_01/CPU.png)

### GPU: Throughput-Oriented Design ğŸŒŠ

- âœ… Many small ALUs
- âœ… Long latency, high throughput operations
- âœ… Heavily pipelined for further throughput enhancement
- âœ… Small **caches** (more die area dedicated to computation)
- âœ… Simple **control** logic (more area dedicated to computation)
- âœ… Massive number of threads to hide very high latency
- âœ… Moderate clock frequency

![GPU Design](../materials/images/Chapter_01/GPU.png)

## ğŸ“± GPU Origins and Evolution

### From Graphics to General Purpose Computing ğŸ®

- ğŸ¯ GPUs were originally built for graphics processing (Graphics Processing Unit)
- ğŸ–¼ï¸ Graphics workloads naturally process a massive number of pixels in parallel
- ğŸ§° Before 2007, programming GPUs required graphics APIs like OpenGL and Direct3D
- ğŸ”„ Using GPUs for non-graphics tasks meant reformulating computations as functions that paint pixels

### CUDA Revolution ğŸ’¡

- ğŸš€ In 2007, NVIDIA released CUDA
- ğŸ”“ Provided a programming interface for general-purpose GPU computing (GPGPU)
- ğŸ› ï¸ Required extensions to the GPU architecture to enable more flexible computing

### Why GPUs Dominated Parallel Computing ğŸ“Š

- ğŸ’° Chip development is extremely expensive and requires high-volume sales
- ğŸ® GPUs already had a large *installed base* from the gaming market
- ğŸ This gave them a significant head start compared to other parallel accelerators
- ğŸ“ˆ Economy of scale made GPUs cost-effective for scientific and data processing applications

## âš–ï¸ Amdahl's Law: The Limits of Parallelization

### The Parallel Computing Pitfall ğŸš§

Consider this example:
- Sequential execution time: **100** seconds
- Fraction of execution that is parallelizable: **90%**
- Speedup achieved on parallelizable part: **1000Ã—**

What's the overall speedup?

```
t_parallel = (1-0.9)*100s + (0.9*100s)/1000 = 10s + 0.09s = 10.09s
speedup = t_sequential/t_parallel = 100s/10.09s = 9.91Ã—
```

### Amdahl's Law Formalized ğŸ“

For an application with:
- **t**: sequential execution time
- **p**: fraction of execution that is parallelizable
- **s**: speedup achieved on the parallelizable part

The overall speedup is:

```
t_parallel = (1-p)*t + (p*t)/s = (1-p+p/s)*t
speedup = t_sequential/t_parallel = t/((1-p+p/s)*t) = 1/(1-p+p/s)
```

As **s** approaches infinity, speedup approaches **1/(1-p)**

![Amdahl's Law](https://upload.wikimedia.org/wikipedia/commons/e/ea/AmdahlsLaw.svg)

### Implications of Amdahl's Law ğŸ”

- âš ï¸ Maximum speedup is limited by the fraction of execution that can't be parallelized
- ğŸ§® If **p** is 90%, speedup < 10Ã— (no matter how many processors you add)
- ğŸŒŸ For many real applications, especially with large datasets, **p** > 99%
- ğŸš€ This enables speedups >100Ã— for data-intensive applications

### Gustafson's Law: The Optimistic View ğŸŒˆ

While Amdahl's Law seems pessimistic, Gustafson's Law suggests that as problem sizes grow, the parallel portion typically grows faster than the sequential portion, allowing for better scalability with larger problems.

## ğŸ“š References

- Programming Massively Parallel Processors (PMPP)
- Computer Architecture: A Quantitative Approach
- NVIDIA CUDA Programming Guide
