# üöÄ 100 Days of GPU Challenge

Welcome to my **100 Days of GPU** journey! This repository will serve as a public log of my learning, experiments, and projects as I dive deep into the world of GPU architecture, CUDA programming, memory hierarchies, parallelism, and acceleration for deep learning and scientific computing.

The goal is to gain both theoretical and hands-on understanding of how GPUs work and how to fully leverage their power for high-performance computing.

üßë‚Äçüè´ **Mentor**: [@hkproj](https://github.com/hkproj) | üìò [100-days-of-gpu](https://github.com/hkproj/100-days-of-gpu)

# Progress Table  
[![YouTube](https://img.shields.io/badge/YouTube-CS149-FF0000?logo=youtube)](https://www.youtube.com/playlist?list=PLoROMvodv4rMp7MTFr4hQsDEcX7Bx6Odp) [![YouTube](https://img.shields.io/badge/YouTube-PMPP-FF0000?logo=youtube)](https://www.youtube.com/playlist?list=PLRRuQYjFhpmubuwx-w8X964ofVkW1T8O4) [![CUDA](https://img.shields.io/badge/CUDA-C++%20Guide-76B900?logo=nvidia)](https://docs.nvidia.com/cuda/cuda-c-programming-guide/) [![PMPP](https://img.shields.io/badge/Book-PMPP-blue?logo=bookstack)](https://github.com/bikrammajhi/100-days-of-GPU/blob/main/materials/Wen-mei%20W.%20Hwu%2C%20David%20B.%20Kirk%2C%20Izzat%20El%20Hajj%20-%20Programming%20Massively%20Parallel%20Processors.%20A%20Hands-on%20Approach-Elsevier%20(2023).pdf)  [![GitHub](https://img.shields.io/badge/GitHub-LeetCUDA-black?logo=github)](https://github.com/xlite-dev/LeetCUDA) [![Website](https://img.shields.io/badge/Website-Operating%20Systems-0A66C2?logo=Google%20Chrome&logoColor=white)](https://www.cse.iitb.ac.in/~mythili/os/) [![YouTube](https://img.shields.io/badge/YouTube-Operating%20Systems-FF0000?logo=youtube)](https://www.youtube.com/playlist?list=PLDW872573QAb4bj0URobvQTD41IV6gRkx) [![Website](https://img.shields.io/badge/Website-DECS-0A66C2?logo=Google%20Chrome&logoColor=white)](https://www.cse.iitb.ac.in/~mythili/decs/) [![YouTube](https://img.shields.io/badge/YouTube-DECS-FF0000?logo=youtube)](https://www.youtube.com/playlist?list=PLOzRYVm0a65dAAfy0d4aRtj5v0OCAvoCY) [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/bikrammajhi/59ee47f2dc6a04fb79f8fa15d498a4bf/hello-world-in-cuda.ipynb)

| Day | üìã Topic | üéØ Key Learning Areas | üíª Implementation |
|:----|:---------|:----------------------|:------------------|
| **001** | üñ•Ô∏è **CPU vs. GPU Architectures & Parallelism** | ‚Ä¢ Processor trends and Moore's Law<br/>‚Ä¢ Latency vs. Throughput-oriented design<br/>‚Ä¢ GPU evolution from graphics to general computing<br/>‚Ä¢ Parallelization limitations | [![View on GitHub](https://img.shields.io/badge/GitHub-Theoretical_study-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20001_%20GPU%20vs%20CPU%20architecture) |
| **002** | üèóÔ∏è **GPU Architecture Fundamentals** | ‚Ä¢ GPU architecture evolution (Fermi ‚Üí Ampere)<br/>‚Ä¢ Streaming Multiprocessors (SMs)<br/>‚Ä¢ Warp execution and scheduling<br/>‚Ä¢ Memory hierarchy (Shared, L1, L2)<br/>‚Ä¢ Tensor Cores for matrix acceleration | [![View on GitHub](https://img.shields.io/badge/GitHub-Hello_World_CUDA-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20002_Hello_GPU) |
| **003** | ‚ûï **Vector Addition** | ‚Ä¢ Data vs. Task parallelism<br/>‚Ä¢ CUDA memory management (`cudaMalloc`, `cudaMemcpy`)<br/>‚Ä¢ Kernel fundamentals and thread indexing<br/>‚Ä¢ Error checking and synchronization<br/>‚Ä¢ Function qualifiers (`__global__`, `__device__`) | [![View on GitHub](https://img.shields.io/badge/GitHub-Vector_addition_kernel-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20003_Vector_Addition) |
| **004** | üåà **Multidimensional Grids and Data** | ‚Ä¢ 2D/3D thread organization<br/>‚Ä¢ Multidimensional indexing techniques<br/>‚Ä¢ Converting 2D coordinates to linear memory<br/>‚Ä¢ Boundary condition handling | [![View on GitHub](https://img.shields.io/badge/GitHub-RGB_to_Grayscale-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20004_Multidimensional_Grids_and_Data) |
| **005** | üñºÔ∏è **Image Blur Processing & Performance** | ‚Ä¢ 3√ó3 average filter implementation<br/>‚Ä¢ Memory transfer vs. computation analysis<br/>‚Ä¢ Performance optimization strategies<br/>‚Ä¢ Shared memory patterns | [![View on GitHub](https://img.shields.io/badge/GitHub-Image_blur_analysis-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20005_Image_Blur) |
| **006** | üî¢ **CUDA Programming Model & Matrix Multiplication** | ‚Ä¢ Scalable parallelism hierarchy<br/>‚Ä¢ Grid Block Clusters and Thread Block Clusters<br/>‚Ä¢ Asynchronous SIMT programming<br/>‚Ä¢ Compute Capability features<br/>‚Ä¢ Memory management techniques | [![View on GitHub](https://img.shields.io/badge/GitHub-Naive_Matrix_Multiplication-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20006_Naive_MatMul) |
| **007** | üß† **L2 Cache and Shared Memory** | ‚Ä¢ L2 Cache control and architecture<br/>‚Ä¢ Memory access pattern optimization<br/>‚Ä¢ Hit ratio strategies<br/>‚Ä¢ L2 cache reset options<br/>‚Ä¢ Set-aside memory layout | [![View on GitHub](https://img.shields.io/badge/GitHub-Tiled_Matrix_Multiplication-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20007_L2%20and%20Shared%20Memory) |
| **008** | üöÑ **Memory Transfer Performance** | ‚Ä¢ Memory types (Pageable, Pinned, Unified)<br/>‚Ä¢ Transfer bandwidth analysis<br/>‚Ä¢ PCIe efficiency optimization<br/>‚Ä¢ Async transfer techniques<br/>‚Ä¢ Batching and memory pools | [![View on GitHub](https://img.shields.io/badge/GitHub-Memory_transfer_benchmarking-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20008_Data_Transfer%20_Benchmark) |
| **009** | üîÑ **Page-Locked Memory & Thread Coarsening** | ‚Ä¢ Page-locked host memory benefits<br/>‚Ä¢ Portable and write-combining memory<br/>‚Ä¢ Mapped memory techniques<br/>‚Ä¢ Thread coarsening optimization strategies | [![View on GitHub](https://img.shields.io/badge/GitHub-Thread_Coarsening_MatMul-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20009_Thread%20Coarsening) |
| **010** | üîÑ **Memory Synchronization Domains** | ‚Ä¢ Memory fence interference handling<br/>‚Ä¢ Traffic isolation with domains<br/>‚Ä¢ Domain usage in CUDA<br/>‚Ä¢ Introduction to Triton programming | [![View on GitHub](https://img.shields.io/badge/GitHub-Vector_Addition_Triton-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20010_Memory%20Synchronization%20Domains) |
| **011** | ‚öôÔ∏è **Asynchronous & Concurrent Execution** | ‚Ä¢ Vector Hadamard Product<br/>‚Ä¢ Concurrent execution between host and device<br/>‚Ä¢ Concurrent kernel execution<br/>‚Ä¢ Overlap of data transfer and kernel execution<br/>‚Ä¢ Concurrent data transfers<br/>‚Ä¢ CUDA streams<br/>‚Ä¢ Stream synchronization<br/>‚Ä¢ Host functions (callbacks)<br/>‚Ä¢ Stream priorities<br/>‚Ä¢ Programmatic dependent launch | [![View on GitHub](https://img.shields.io/badge/GitHub-Vector_Hadamard_Product-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/blob/main/Day%20011_Asynchronous%20Concurrent%20Execution/vectorHadamard.cu) |
| **012** | üñ•Ô∏è **Multi-Device System** | ‚Ä¢ Device enumeration and selection<br/>‚Ä¢ Stream and event behavior<br/>‚Ä¢ Peer-to-peer memory access and copy<br/>‚Ä¢ Unified Virtual Address Space<br/>‚Ä¢ Interprocess communication (IPC)<br/>‚Ä¢ Error checking in CUDA | [![View on GitHub](https://img.shields.io/badge/GitHub-Vector_Dot_Product_Atomic-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20012_Multi-Device%20System) |
| **013** | üîÑ **CUDA Versioning & Compatibility** | ‚Ä¢ CUDA version compatibility rules<br/>‚Ä¢ Mix-and-match versioning between driver and runtime<br/>‚Ä¢ Compute mode settings and switching<br/>‚Ä¢ Understanding compatibility modes<br/>‚Ä¢ Naive Softmax implementation | [![View on GitHub](https://img.shields.io/badge/GitHub-Naive_Softmax-black?logo=github&style=flat-square)](https://github.com/bikrammajhi/100-days-of-GPU/tree/main/Day%20013_CUDA%20Versioning_Compatibility%20and%20Modes) |
# References:
[![GitHub](https://img.shields.io/badge/GitHub-LeetCUDA-181717?logo=github)](https://github.com/xlite-dev/LeetCUDA) [![GitHub](https://img.shields.io/badge/GitHub-CUDA%20Optimizations-181717?logo=github)](https://github.com/BBuf/how-to-optim-algorithm-in-cuda) [![Awesome](https://img.shields.io/badge/Awesome-CUDA%20&%20HPC-ff6b6b?logo=awesome-lists)](https://github.com/coderonion/awesome-cuda-and-hpc) [![Awesome](https://img.shields.io/badge/Awesome-DiT%20Inference-4ecdc4?logo=awesome-lists)](https://github.com/xlite-dev/Awesome-DiT-Inference) [![GitHub](https://img.shields.io/badge/GitHub-Stable%20Diffusion%20C++-181717?logo=github)](https://github.com/leejet/stable-diffusion.cpp) 

[![Blog](https://img.shields.io/badge/Blog-Tensor%20Core%20MatMul-e91e63?logo=hashnode)](https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html) 
[![Blog](https://img.shields.io/badge/Blog-CUDA%20Basics-2196f3?logo=medium)](https://tinkerd.net/blog/machine-learning/cuda-basics/#wrapping-up)
[![Blog](https://img.shields.io/badge/Blog-MMA%20MatMul-2196f3?logo=medium)](https://www.spatters.ca/mma-matmul) 
[![Blog](https://img.shields.io/badge/Blog-CUDA%20MatMul%20Optimization-00d4aa?logo=dev.to)](https://siboehm.com/articles/22/CUDA-MMM) 
[![Substack](https://img.shields.io/badge/Substack-Outperforming%20cuBLAS-ff6b35?logo=substack)](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog) 
[![YouTube](https://img.shields.io/badge/YouTube-Outperforming%20cuBLAS-FF0000?logo=youtube)](https://www.youtube.com/watch?v=ErTmTCRP1_U) 

[![Blog](https://img.shields.io/badge/Blog-Optimizing%20LayerNorm-2196f3?logo=medium)](https://aryagxr.com/blogs/cuda-optimizing-layernorm)



---

 




